{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NMTDataset as NMTDataset\n",
    "import NMTEncoder as NMTEncoder\n",
    "import NMTDecoder as NMTDecoder\n",
    "import NMTModel as NMTModel\n",
    "import NMTSampler as NMTSampler\n",
    "import NMTVectorizer as NMTVectorizer\n",
    "import SequenceVocabulary as SequenceVocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating minibatches for NMT\n",
    "\n",
    "def generate_nmt_batches(dataset, batch_size, shuffle=True,\n",
    "                         drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the Pytorch DataLoader; NMT version\n",
    "    \"\"\"\n",
    "\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        lengths = data_dict['x_source_length'].numpy()\n",
    "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
    "\n",
    "        out_data_dict = {}\n",
    "        for name , tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\n",
    "\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def verbose_attention(encoder_state_vectors, query_vector):\n",
    "    \"\"\"A descriptive version of the neural attention mechanism \n",
    "    \n",
    "    Args:\n",
    "        encoder_state_vectors (torch.Tensor): 3dim tensor from bi-GRU in encoder\n",
    "        query_vector (torch.Tensor): hidden state in decoder GRU\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    batch_size , num_vectors, vector_size = encoder_state_vectors.size()\n",
    "    vector_scores = torch.sum(encoder_state_vectors * query_vector.view(batch_size, 1, ))\n",
    "\n",
    "    vector_probabilities = F.softmax(vector_scores, dim=1)\n",
    "    weighted_vectors = encoder_state_vectors * vector_probabilities.view(batch_size, )\n",
    "\n",
    "    context_vectors = torch.sum(weighted_vectors, dim=1)\n",
    "\n",
    "    return context_vectors, vector_probabilities, vector_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def terse_attention(encoder_state_vectors, query_vector):\n",
    "    \"\"\"A shorter and more optimized version of the neural attention mechanism\n",
    "    \n",
    "    Args:\n",
    "        encoder_state_vectors (torch.Tensor): 3dim tensor from bi-GRU in encoder\n",
    "        query_vector (torch.Tensor): hidden state\n",
    "    \"\"\"\n",
    "    vector_scores = torch.matmul(encoder_state_vectors, query_vector.unsqueze(dim=2))\n",
    "\n",
    "    vector_probabilities = F.softmax(vector_scores, dim=-1)\n",
    "    context_vectors = torch.matmul(encoder_state_vectors.transpose(-2, -1),\n",
    "                                   vector_probabilities.unsqueeze(dim=2)).squeeze()\n",
    "    \n",
    "    return context_vectors, vector_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    dataset_csv=\"data/nmt/simplest_eng_fra.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/ch8/nmt_luong_sampling\",\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    cuda=True,\n",
    "    seed=1337,\n",
    "    learning_rate=5e-4,\n",
    "    batch_size=32,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    source_embedding_size=24,\n",
    "    target_embedding_size=24,\n",
    "    encoding_size=32,\n",
    "    catch_keyboard_interrupt=True\n",
    ")\n",
    "\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "    \n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expand filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "    # check CUDA\n",
    "    if not torch.cuda.is_available():\n",
    "        args.cuda = False\n",
    "\n",
    "    args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "    # set seed for reproducibility\n",
    "    set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "    # handle dirs\n",
    "    handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "    # training from a checkpoint\n",
    "    dataset = NMTDataset.load_dataset_and_load_vectorizer(args.dataset_csv,\n",
    "                                                          args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = NMTDataset.load_dataset_and_make_vectorizer(args.dataset_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMTModel(source_vocab_size=len(vectorizer.source_vocab),\n",
    "                 source_embedding_size=args.source_embedding_size,\n",
    "                 target_vocab_size=len(vectorizer.target_vocab),\n",
    "                 target_embedding_size=args.target_embedding_size,\n",
    "                 encoding_size=args.encoding_size,\n",
    "                 target_bos_index=vectorizer.target_vocab.begin_seq_index)\n",
    "\n",
    "\n",
    "if args.reload_from_files and os.path.exist(args.model_state_file):\n",
    "    model.load_state_dict(torch.load(args.model_state_file))\n",
    "    print(\"Reload model\")\n",
    "\n",
    "else:\n",
    "    print(\"New Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(args.device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "mask_index = vectorizer.target_vocab.mask_index\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='training routine',\n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size),\n",
    "                          position=1,\n",
    "                          leave=True)\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size),\n",
    "                        position=1,\n",
    "                        leave=True)\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        sample_probability = (20 + epoch_index) / args.num_epochs\n",
    "\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # Setup: batch generator , set loss and acc to 0, set train mode on \n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_nmt_batches(dataset,\n",
    "                                               batch_size=args.batch_size,\n",
    "                                               device=args.device)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is the 5 steps:\n",
    "            # Step 1. Zero the gradient \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2, compute the outputs\n",
    "            y_pred = model(batch_dict['x_source'],\n",
    "                           batch_dict['x_source_length'],\n",
    "                           batch_dict['x_target'],\n",
    "                           sample_probability=sample_probability)\n",
    "            \n",
    "            # step 3: compute the loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "            # step 4: use loss tp produce gradients \n",
    "            loss.backwards()\n",
    "\n",
    "            # step 5: use the optimizer to take gradient step \n",
    "            optimizer.step()\n",
    "\n",
    "            # compute the runninng loss and running accuarcy\n",
    "\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss,\n",
    "                                  acc=running_acc,\n",
    "                                  epoch=epoch_index)\n",
    "\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch_generator ,set loss and acc to 0 ; set eval mode on\n",
    "\n",
    "        dataset.split('val')\n",
    "        batch_generator = generate_nmt_batches(dataset,\n",
    "                                               batch_size=args.batch_size,\n",
    "                                               device=args.device)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "\n",
    "            y_pred = model(batch_dict['x_source'],\n",
    "                           batch_dict['x_source_length'],\n",
    "                           batch_dict['x_target'],\n",
    "                           sample_probability=sample_probability)\n",
    "            \n",
    "            # step 3: compute the loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "            # compute the running loss and accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args.args, model=model, train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.set_postfix(best_val=train_state['early_stopping_best_val'])\n",
    "        epoch_bar.update()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "chencherry = bleu_score.SmoothingFunection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_from_indices(indices, vocab, strict=True, return_string=True):\n",
    "    ignore_indices = set([vocab.mask_index, vocab.begin_seq_index, vocab.end_seq_index])\n",
    "    out = []\n",
    "    for index in indices:\n",
    "        if index == vocab.begin_seq_index and strict:\n",
    "            continue\n",
    "        elif index == vocab.end_seq_index and strict:\n",
    "            break\n",
    "        else:\n",
    "            out.append(vocab.lookup_index(index))\n",
    "\n",
    "    if return_string:\n",
    "        return \" \".join(out)\n",
    "    else:\n",
    "        return out\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval().to(args.device)\n",
    "\n",
    "sampler = NMTSampler(vectorizer, model)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_nmt_batches(dataset,\n",
    "                                       batch_size=args.batch_size,\n",
    "                                       device=args.device)\n",
    "\n",
    "test_results = []\n",
    "for batch_dict in batch_generator:\n",
    "    sampler.apply_to_batch(batch_dict)\n",
    "    for i in range(args.batch_size):\n",
    "        test_results.append(sampler.get_ith_item(i, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([r['bleu-4'] for r in test_results], bins=100)\n",
    "np.mean([r['bleu-4'] for r in test_results]) , np.median([r['bleu-4'] for r in test_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('val')\n",
    "batch_generator = generate_nmt_batches(dataset,\n",
    "                                       batch_size=args.batch_size,\n",
    "                                       device=args.device)\n",
    "\n",
    "batch_dict = next(batch_generator)\n",
    "\n",
    "model = model.eval().to(args.device)\n",
    "sampler = NMTSampler(vectorizer.model)\n",
    "sampler.apply_to_batch(batch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for i in range(args.batch_size):\n",
    "    all_results.append(sampler.get_ith_item(i, False))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = [x for x in all_results if x['bleu-4'] > 0.5]\n",
    "len(top_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in top_results:\n",
    "    plt.figure()\n",
    "    target_len = len(sample['sampled'])\n",
    "    source_len = len(sample['source'])\n",
    "\n",
    "    attention_matrix = sample['attention'][:target_len, :source_len+2].transpose()\n",
    "    ax = sns.heatmap(attention_matrix, center=0.0)\n",
    "    ylabs = [\"<BOS>\"]+sample['source']+[\"<EOS>\"]\n",
    "\n",
    "    ax.set_yticklabels(ylabs, rotation=0)\n",
    "    ax.set_xtickslabels(sample['sampled'], rotation=90)\n",
    "    ax.set_xlabel(\"Target Sentence\")\n",
    "    ax.set_ylabels(\"Source Sentence\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_sentence(vectorizer, batch_dict, index):\n",
    "    indices = batch_dict['x_source'][index].cpu().numpy()\n",
    "    vocab = vectorizer.source_vocab\n",
    "    return sentence_from_indices(indices, vocab)\n",
    "\n",
    "def get_true_sentence(vectorizer, batch_dict, index):\n",
    "    return sentence_from_indices(batch_dict['y_target'].cpu().data.numpy()[index], vectorizer.target_vocab)\n",
    "\n",
    "def get_sampled_sentence(vectorizer, batch_dict, index):\n",
    "    y_pred = model(x_source=batch_dict['x_source'],\n",
    "                   x_source_lengths=batch_dict['x_source_length'],\n",
    "                   target_sequence=batch_dict['x_target'],\n",
    "                   sample_probability=1.0)\n",
    "    return sentence_from_indices(torch.max(y_pred, dim=2)[1].cpu().data.numpy()[index], vectorizer.target_vocab)\n",
    "\n",
    "def get_all_sentences(vectorizer, batch_dict, index):\n",
    "    return {\n",
    "        \"source\": get_source_sentence(vectorizer, batch_dict, index),\n",
    "        \"truth\": get_true_sentence(vectorizer, batch_dict, index),\n",
    "        \"sampled\": get_sampled_sentence(vectorizer, batch_dict, index)\n",
    "    }\n",
    "\n",
    "def sentence_from_indices(indices, vocab, strict=True):\n",
    "    ignore_indices = set([vocab.mask_index, vocab.begin_seq_index, vocab.end_seq_index])\n",
    "    out = []\n",
    "    for index in indices:\n",
    "        if index == vocab.begin_seq_index and strict:\n",
    "            continue\n",
    "        elif index == vocab.end_seq_index and strict:\n",
    "            return \" \".join(out)\n",
    "        else:\n",
    "            out.append(vocab.lookup_index(index))\n",
    "    return \" \".join(out)\n",
    "\n",
    "results = get_all_sentences(vectorizer, batch_dict, 1)\n",
    "results\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
